{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "bd23d7f3",
            "metadata": {},
            "source": [
                "# üöÄ Google Colab GPU Runtime Verification & Configuration\n",
                "\n",
                "This notebook verifies and configures the Google Colab runtime for ML/DL workflows.\n",
                "\n",
                "**Steps:**\n",
                "1. GPU Detection & Type Identification\n",
                "2. CUDA/cuDNN Compatibility Check\n",
                "3. System Info (Python, RAM)\n",
                "4. Library Installation & Verification\n",
                "5. Reproducibility Seeds\n",
                "6. Environment Summary Report"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c7676f1",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ GPU Detection & nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "70dd120a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "NVIDIA-SMI Output\n",
                        "============================================================\n",
                        "Wed Jan  7 07:12:35 2026       \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
                        "|-----------------------------------------+------------------------+----------------------+\n",
                        "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
                        "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
                        "|                                         |                        |               MIG M. |\n",
                        "|=========================================+========================+======================|\n",
                        "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
                        "| N/A   71C    P0             32W /   70W |     120MiB /  15360MiB |      0%      Default |\n",
                        "|                                         |                        |                  N/A |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "                                                                                         \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| Processes:                                                                              |\n",
                        "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
                        "|        ID   ID                                                               Usage      |\n",
                        "|=========================================================================================|\n",
                        "+-----------------------------------------------------------------------------------------+\n"
                    ]
                }
            ],
            "source": [
                "# Check GPU availability using nvidia-smi\n",
                "print(\"=\" * 60)\n",
                "print(\"NVIDIA-SMI Output\")\n",
                "print(\"=\" * 60)\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "98b5d1e8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "PyTorch GPU Detection\n",
                        "============================================================\n",
                        "CUDA Available: True\n",
                        "Number of GPUs: 1\n",
                        "\n",
                        "GPU 0: Tesla T4\n",
                        "  Compute Capability: 7.5\n",
                        "  Total VRAM: 14.74 GB\n"
                    ]
                }
            ],
            "source": [
                "# GPU Detection with PyTorch\n",
                "import torch\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"PyTorch GPU Detection\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "cuda_available = torch.cuda.is_available()\n",
                "print(f\"CUDA Available: {cuda_available}\")\n",
                "\n",
                "if cuda_available:\n",
                "    gpu_count = torch.cuda.device_count()\n",
                "    print(f\"Number of GPUs: {gpu_count}\")\n",
                "    \n",
                "    for i in range(gpu_count):\n",
                "        gpu_name = torch.cuda.get_device_name(i)\n",
                "        gpu_capability = torch.cuda.get_device_capability(i)\n",
                "        print(f\"\\nGPU {i}: {gpu_name}\")\n",
                "        print(f\"  Compute Capability: {gpu_capability[0]}.{gpu_capability[1]}\")\n",
                "        \n",
                "        # Memory info\n",
                "        total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
                "        print(f\"  Total VRAM: {total_memory:.2f} GB\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No GPU detected. Please enable GPU runtime in Colab:\")\n",
                "    print(\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "66dbc220",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ CUDA & cuDNN Compatibility Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "2da70e1a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "CUDA & cuDNN Verification\n",
                        "============================================================\n",
                        "PyTorch CUDA Version: 12.6\n",
                        "cuDNN Version: 91002\n",
                        "cuDNN Enabled: True\n",
                        "\n",
                        "--- NVCC Version ---\n",
                        "nvcc: NVIDIA (R) Cuda compiler driver\n",
                        "Copyright (c) 2005-2024 NVIDIA Corporation\n",
                        "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
                        "Cuda compilation tools, release 12.5, V12.5.82\n",
                        "Build cuda_12.5.r12.5/compiler.34385749_0\n",
                        "\n",
                        "‚úÖ CUDA Test Passed: [2. 4. 6.]\n"
                    ]
                }
            ],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"CUDA & cuDNN Verification\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# CUDA Version from PyTorch\n",
                "print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
                "\n",
                "# cuDNN Version\n",
                "cudnn_version = torch.backends.cudnn.version()\n",
                "print(f\"cuDNN Version: {cudnn_version}\")\n",
                "print(f\"cuDNN Enabled: {torch.backends.cudnn.enabled}\")\n",
                "\n",
                "# Check CUDA compilation tools\n",
                "print(\"\\n--- NVCC Version ---\")\n",
                "!nvcc --version 2>/dev/null || echo \"nvcc not found (optional)\"\n",
                "\n",
                "# Verify CUDA is functional\n",
                "if torch.cuda.is_available():\n",
                "    try:\n",
                "        test_tensor = torch.tensor([1.0, 2.0, 3.0]).cuda()\n",
                "        result = test_tensor * 2\n",
                "        print(f\"\\n‚úÖ CUDA Test Passed: {result.cpu().numpy()}\")\n",
                "    except Exception as e:\n",
                "        print(f\"\\n‚ùå CUDA Test Failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2fa89721",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Python Version & System RAM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "28e2024b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "System Information\n",
                        "============================================================\n",
                        "Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
                        "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
                        "\n",
                        "CPU Cores: 2\n",
                        "\n",
                        "Total RAM: 12.67 GB\n",
                        "Available RAM: 11.13 GB\n",
                        "Used RAM: 1.22 GB (12.2%)\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import platform\n",
                "import psutil\n",
                "import os\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"System Information\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Python Version\n",
                "print(f\"Python Version: {sys.version}\")\n",
                "print(f\"Platform: {platform.platform()}\")\n",
                "\n",
                "# CPU Information\n",
                "cpu_count = os.cpu_count()\n",
                "print(f\"\\nCPU Cores: {cpu_count}\")\n",
                "\n",
                "# RAM Information\n",
                "ram_info = psutil.virtual_memory()\n",
                "total_ram_gb = ram_info.total / (1024**3)\n",
                "available_ram_gb = ram_info.available / (1024**3)\n",
                "used_ram_gb = ram_info.used / (1024**3)\n",
                "\n",
                "print(f\"\\nTotal RAM: {total_ram_gb:.2f} GB\")\n",
                "print(f\"Available RAM: {available_ram_gb:.2f} GB\")\n",
                "print(f\"Used RAM: {used_ram_gb:.2f} GB ({ram_info.percent}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d5c4128a",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Install & Verify Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "9e51afc2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "Library Version Verification\n",
                        "============================================================\n",
                        "NumPy           : 2.0.2\n",
                        "Pandas          : 2.2.2\n",
                        "SciPy           : 1.16.3\n",
                        "Scikit-learn    : 1.6.1\n",
                        "PyTorch         : 2.9.0+cu126\n",
                        "TorchVision     : 0.24.0+cu126\n",
                        "Matplotlib      : 3.10.0\n",
                        "Seaborn         : 0.13.2\n",
                        "tqdm            : 4.67.1\n",
                        "\n",
                        "PyTorch CUDA    : True\n",
                        "PyTorch cuDNN   : True\n"
                    ]
                }
            ],
            "source": [
                "# Verify all library installations\n",
                "print(\"=\" * 60)\n",
                "print(\"Library Version Verification\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import scipy\n",
                "import sklearn\n",
                "import torch\n",
                "import torchvision\n",
                "import matplotlib\n",
                "import seaborn as sns\n",
                "import tqdm\n",
                "\n",
                "libraries = {\n",
                "    \"NumPy\": np.__version__,\n",
                "    \"Pandas\": pd.__version__,\n",
                "    \"SciPy\": scipy.__version__,\n",
                "    \"Scikit-learn\": sklearn.__version__,\n",
                "    \"PyTorch\": torch.__version__,\n",
                "    \"TorchVision\": torchvision.__version__,\n",
                "    \"Matplotlib\": matplotlib.__version__,\n",
                "    \"Seaborn\": sns.__version__,\n",
                "    \"tqdm\": tqdm.__version__,\n",
                "}\n",
                "\n",
                "for lib, version in libraries.items():\n",
                "    print(f\"{lib:15} : {version}\")\n",
                "\n",
                "# Verify PyTorch CUDA support\n",
                "print(f\"\\n{'PyTorch CUDA':15} : {torch.cuda.is_available()}\")\n",
                "print(f\"{'PyTorch cuDNN':15} : {torch.backends.cudnn.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bba176b7",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Set Global Random Seeds for Reproducibility"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "068e6b55",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Random seeds set to 42 for reproducibility\n",
                        "   - Python random: 42\n",
                        "   - NumPy: 42\n",
                        "   - PyTorch: 42\n",
                        "   - CUDA: 42\n",
                        "   - cuDNN deterministic: True\n"
                    ]
                }
            ],
            "source": [
                "import random\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "def set_seed(seed: int = 42):\n",
                "    \"\"\"\n",
                "    Set random seeds for reproducibility across all libraries.\n",
                "    \n",
                "    Args:\n",
                "        seed: Random seed value (default: 42)\n",
                "    \"\"\"\n",
                "    # Python random\n",
                "    random.seed(seed)\n",
                "    \n",
                "    # NumPy\n",
                "    np.random.seed(seed)\n",
                "    \n",
                "    # PyTorch\n",
                "    torch.manual_seed(seed)\n",
                "    \n",
                "    # CUDA seeds (if available)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)\n",
                "        torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
                "        \n",
                "        # Ensure deterministic behavior (may impact performance)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "        torch.backends.cudnn.benchmark = False\n",
                "    \n",
                "    print(f\"‚úÖ Random seeds set to {seed} for reproducibility\")\n",
                "    print(f\"   - Python random: {seed}\")\n",
                "    print(f\"   - NumPy: {seed}\")\n",
                "    print(f\"   - PyTorch: {seed}\")\n",
                "    if torch.cuda.is_available():\n",
                "        print(f\"   - CUDA: {seed}\")\n",
                "        print(f\"   - cuDNN deterministic: True\")\n",
                "\n",
                "# Set the seed\n",
                "SEED = 42\n",
                "set_seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "17a6c503",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Environment Summary Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "d187c582",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "üñ•Ô∏è  ENVIRONMENT SUMMARY REPORT\n",
                        "============================================================\n",
                        "\n",
                        "üìä GPU Configuration\n",
                        "----------------------------------------\n",
                        "  GPU Name      : Tesla T4\n",
                        "  CUDA Version  : 12.6\n",
                        "  Total VRAM    : 14.74 GB\n",
                        "  Free VRAM     : 14.74 GB\n",
                        "  Allocated     : 0.0000 GB\n",
                        "\n",
                        "üî• PyTorch Configuration\n",
                        "----------------------------------------\n",
                        "  PyTorch Version : 2.9.0+cu126\n",
                        "  CUDA Available  : True\n",
                        "  cuDNN Available : True\n",
                        "  cuDNN Version   : 91002\n",
                        "\n",
                        "üíª System Configuration\n",
                        "----------------------------------------\n",
                        "  Python Version  : 3.12.12\n",
                        "  CPU Cores       : 2\n",
                        "  Total RAM       : 12.67 GB\n",
                        "  Available RAM   : 11.13 GB\n",
                        "  RAM Usage       : 12.2%\n",
                        "\n",
                        "============================================================\n",
                        "üîç FINAL CUDA AVAILABILITY CHECK\n",
                        "============================================================\n",
                        "\n",
                        "  ‚úÖ CUDA is AVAILABLE and ready for use!\n",
                        "     Device: Tesla T4\n",
                        "\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "import psutil\n",
                "import torch\n",
                "\n",
                "def generate_environment_report():\n",
                "    \"\"\"\n",
                "    Generate a clean environment summary report.\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"üñ•Ô∏è  ENVIRONMENT SUMMARY REPORT\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # GPU Information\n",
                "    print(\"\\nüìä GPU Configuration\")\n",
                "    print(\"-\" * 40)\n",
                "    if torch.cuda.is_available():\n",
                "        gpu_name = torch.cuda.get_device_name(0)\n",
                "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
                "        cuda_version = torch.version.cuda\n",
                "        \n",
                "        # Get current GPU memory usage\n",
                "        allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
                "        reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
                "        free_vram = gpu_memory - reserved\n",
                "        \n",
                "        print(f\"  GPU Name      : {gpu_name}\")\n",
                "        print(f\"  CUDA Version  : {cuda_version}\")\n",
                "        print(f\"  Total VRAM    : {gpu_memory:.2f} GB\")\n",
                "        print(f\"  Free VRAM     : {free_vram:.2f} GB\")\n",
                "        print(f\"  Allocated     : {allocated:.4f} GB\")\n",
                "    else:\n",
                "        print(\"  ‚ö†Ô∏è No GPU available\")\n",
                "    \n",
                "    # PyTorch Information\n",
                "    print(\"\\nüî• PyTorch Configuration\")\n",
                "    print(\"-\" * 40)\n",
                "    print(f\"  PyTorch Version : {torch.__version__}\")\n",
                "    print(f\"  CUDA Available  : {torch.cuda.is_available()}\")\n",
                "    print(f\"  cuDNN Available : {torch.backends.cudnn.is_available()}\")\n",
                "    if torch.backends.cudnn.is_available():\n",
                "        print(f\"  cuDNN Version   : {torch.backends.cudnn.version()}\")\n",
                "    \n",
                "    # CPU & RAM Information\n",
                "    print(\"\\nüíª System Configuration\")\n",
                "    print(\"-\" * 40)\n",
                "    print(f\"  Python Version  : {sys.version.split()[0]}\")\n",
                "    print(f\"  CPU Cores       : {os.cpu_count()}\")\n",
                "    \n",
                "    ram = psutil.virtual_memory()\n",
                "    print(f\"  Total RAM       : {ram.total / (1024**3):.2f} GB\")\n",
                "    print(f\"  Available RAM   : {ram.available / (1024**3):.2f} GB\")\n",
                "    print(f\"  RAM Usage       : {ram.percent}%\")\n",
                "    \n",
                "    # Final CUDA Check\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"üîç FINAL CUDA AVAILABILITY CHECK\")\n",
                "    print(\"=\" * 60)\n",
                "    cuda_status = torch.cuda.is_available()\n",
                "    if cuda_status:\n",
                "        print(\"\\n  ‚úÖ CUDA is AVAILABLE and ready for use!\")\n",
                "        print(f\"     Device: {torch.cuda.get_device_name(0)}\")\n",
                "    else:\n",
                "        print(\"\\n  ‚ùå CUDA is NOT available.\")\n",
                "        print(\"     Please enable GPU runtime in Colab.\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    return cuda_status\n",
                "\n",
                "# Generate the report\n",
                "cuda_ready = generate_environment_report()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bde72d11",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ‚úÖ Environment Ready!\n",
                "\n",
                "If all checks passed, your Google Colab GPU runtime is configured and ready for ML/DL workflows.\n",
                "\n",
                "**Next Steps:**\n",
                "- Load your dataset\n",
                "- Define your model architecture\n",
                "- Start training!\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5e238e6f",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
